import argparse
from pprint import pprint
import itertools
import math
import time
import csv

dimension = 15
# one_vector = [1] * dimension
# zero_vector = [0] * dimension


def get_lines (fname):
  inputfile = open (fname, 'r')
  reader = csv.DictReader (inputfile)
  rows = list (reader)
  inputfile.close()
  return rows


def mk_type_count_map (raws):
  count = dict ()

  for raw in raws:
    words = raw.split('-')[1:] # exclude constructor [C] when counting used types

    for word in words:
      feat = word.split(':')
      defs, uses, payable = feat[0].split('@'), feat[1].split('@'), feat[2]
      defs = list (filter (lambda d: d!='', defs))
      uses = list (filter (lambda u: u!='', uses))

      for d in defs:
        count[d] = count.get(d,0) + 1 # if word is not in dictionary, default value is 0.
      for u in uses:
        count[u] = count.get(u,0) + 1

  count = sorted (count.items(), key=(lambda x:x[1]), reverse=True)

  f = open ('./src/exploit/train/typeRank.txt', 'w')
  for (k,v) in count:
    f.write (k + ',' + str(v))
    f.write ('\n')
  f.close ()

  return count


def is_top_k (k, rank, typs):
  res = '0'
  try:
    if rank[k] in typs:
      res = '1'
    else:
      res = '0'
  except IndexError:
    res = '0'

  return res


# ret: string (sentence) list
def preprocess (n, rank, raws):
  sentences = []

  for raw in raws:
    words = raw.split ('-')

    tmp = ''
    for word in words:
      if word=='[C]':
        tmp = tmp + word
      else:
        feat = word.split(':')
        
        defs, uses, payable, send_eth, destruct = feat[0].split('@'), feat[1].split('@'), feat[2], feat[3], feat[4]

        defs = list(filter (lambda d: d!='', defs))
        uses = list(filter (lambda u: u!='', uses))

        vector = []
        vector.append (is_top_k (0, rank, defs))
        vector.append (is_top_k (1, rank, defs))
        vector.append (is_top_k (2, rank, defs))
        vector.append (is_top_k (3, rank, defs))
        vector.append (is_top_k (4, rank, defs))
        vector.append (is_top_k (5, rank, defs))

        vector.append (is_top_k (0, rank, uses))
        vector.append (is_top_k (1, rank, uses))
        vector.append (is_top_k (2, rank, uses))
        vector.append (is_top_k (3, rank, uses))
        vector.append (is_top_k (4, rank, uses))
        vector.append (is_top_k (5, rank, defs))

        vector.append (payable)
        vector.append (send_eth)
        vector.append (destruct)


        tmp = tmp + '-' + '[' + ''.join(vector) + ']'

    # s = (line.replace ('\n','') + ' ' + tmp)
    # print (s) if '[00000000000]' in tmp else ()
    sentences.append (tmp)

  pred = ''
  post = ''
  for i in range (n-1):
    pred = pred + '<S>-'
    post = post + '-<E>'
  sentences = list(map (lambda s: pred + s + post, sentences))
  return sentences


def tokenize (sentence):
  return sentence.split ('-')


def mk_ngram_sentence (n, sentence):
  words = tokenize (sentence)
  res = []
  for i in range (len (words) - n + 1):
    res.append (words[i:i+n])
  return res


def mk_ngram_sentences (n, sentences):
  res = []
  for s in sentences:
    res.extend (mk_ngram_sentence (n,s)) # not append, but extend
  return res


def mk_word_count (sentences):
  count = dict()
  for s in sentences:
    words = tokenize (s)
    for word in tokenize (s):
      count[word] = count.get(word,0) + 1
  return count


def mk_vocabs (dim):
  res = itertools.product (['0','1'], repeat=dim)
  res = list (res)
  res = list (map (lambda e: '[' + ''.join(list(e)) + ']', res)) # 'tuple' -> 'list' -> 'string'
  res.insert (0, '[C]')
  res.insert (0, '<E>')
  res.insert (0, '<S>')
  return res # [<S>, <E>, [C], [0000...], ...]


def mk_ngrams_from_vocabs (n, vocabs):
  res = itertools.product (vocabs, repeat=n)
  res = list (res)
  res = list(map (lambda e: '-'.join(list(e)), res))
  return res


def mk_ngram_count (ngrams):
  count = dict ()
  for ngram in ngrams:
    s = '-'.join(ngram)
    count[s] = count.get(s,0) + 1
  return count


def mle_add_one (ctx_count, ngram_count, vocabs, ctx, ngram):
  vocab_size = len (vocabs)
  prob = (ngram_count.get (ngram, 0) + 1) / ((ctx_count.get (ctx, 0) + vocab_size) * 1.0)
  return prob


def mle_add_k (ctx_count, ngram_count, vocabs, ctx, ngram):
  vocab_size = len (vocabs)
  k = 0.2
  prob = (ngram_count.get (ngram, 0) + k) / ((ctx_count.get (ctx, 0) + (k * vocab_size)) * 1.0)
  return prob


def interpolation_bi (ngram, uni_count, bi_count, tri_count, quad_count, vocabs):
  l1 = 0.9
  l2 = 0.1

  lst = ngram.split('-')
  assert (len(lst) == 2)

  bi = ngram
  bi_ctx = ngram.split('-')[0]
  uni = ngram.split ('-')[1]

  cnt_bi = bi_count.get(ngram,0)
  cnt_uni = uni_count[uni]

  prob_bi = mle_add_k (uni_count, bi_count, vocabs, bi_ctx, bi)
  prob_uni = cnt_uni / sum(uni_count.values())

  n1 = l1 * prob_bi
  n2 = l2 * prob_uni
  return n1+n2


def interpolation_tri (ngram, uni_count, bi_count, tri_count, quad_count, vocabs):
  l1 = 0.9
  l2 = 0.08
  l3 = 0.02

  lst = ngram.split('-')
  assert (len(lst) == 3)

  tri = '-'.join(lst[0:3])
  tri_ctx = '-'.join(lst[0:2])
  bi = '-'.join(lst[1:3])
  bi_ctx = '-'.join(lst[1:2])
  uni = lst[2]

  cnt_tri = tri_count.get(tri,0)
  cnt_bi = bi_count.get(bi,0)
  cnt_uni = uni_count[uni]
  assert (cnt_uni > 0)

  prob_tri = mle_add_k (bi_count, tri_count, vocabs, tri_ctx, tri)
  prob_bi = mle_add_k (uni_count, bi_count, vocabs, bi_ctx, bi)
  prob_uni = cnt_uni / sum(uni_count.values())

  n1 = l1 * prob_tri
  n2 = l2 * prob_bi
  n3 = l3 * prob_uni
  return n1+n2+n3


def interpolation_quad (ngram, uni_count, bi_count, tri_count, quad_count, vocabs):
  l1 = 0.7
  l2 = 0.2
  l3 = 0.08
  l4 = 0.02

  lst = ngram.split('-')
  assert (len(lst) == 4)

  quad, quad_ctx = ngram,              '-'.join(lst[0:3])
  tri,  tri_ctx  = '-'.join(lst[1:4]), '-'.join(lst[1:3])
  bi,   bi_ctx   = '-'.join(lst[2:4]), '-'.join(lst[2:3])
  uni = lst[3]

  cnt_quad = quad_count.get(quad,0)
  cnt_tri = tri_count.get(tri,0)
  cnt_bi = bi_count.get(bi,0) 
  cnt_uni = uni_count[uni]

  prob_quad = mle_add_k (tri_count, quad_count, vocabs, quad_ctx, quad)
  prob_tri  = mle_add_k (bi_count,  tri_count,  vocabs, tri_ctx,  tri)
  prob_bi   = mle_add_k (uni_count, bi_count,   vocabs, bi_ctx,   bi)
  prob_uni  = cnt_uni / sum(uni_count.values())

  n1 = l1 * prob_quad
  n2 = l2 * prob_tri
  n3 = l3 * prob_bi
  n4 = l4 * prob_uni
  return n1+n2+n3+n4


def learn_model (n, out, model, uni_count, bi_count, tri_count, quad_count, vocabs):
  f_model = open (out, 'w')
  f_pmat = open ('./src/exploit/train/prob_matrix.csv', 'w')
  f_cmat = open ('./src/exploit/train/cnt_matrix.csv', 'w')

  f_pmat.write (',')
  f_cmat.write (',')
  for w in vocabs:
    f_pmat.write (w + ',')
    f_cmat.write (w + ',')
  f_pmat.write ('\n')
  f_cmat.write ('\n')

  contexts = mk_ngrams_from_vocabs (n-1, vocabs)

  for (i,ctx) in enumerate (contexts):
    print ('Context : ' + str(i+1) + '/' + str(len(contexts))) if ((i+1) % 100) == 1 else ()
    f_pmat.write (ctx + ',')
    f_cmat.write (ctx + ',')

    for (j,w) in enumerate (vocabs):
      ngram = ctx + '-' + w
      prob = 0.0
      if model == "interpolation":
        if n==2:
          prob = interpolation_bi (ngram, uni_count, bi_count, tri_count, quad_count, vocabs)
        elif n==3:
          prob = interpolation_tri (ngram, uni_count, bi_count, tri_count, quad_count, vocabs)
        elif n==4:
          prob = interpolation_quad (ngram, uni_count, bi_count, tri_count, quad_count, vocabs)
        else:
          raise NotImplementedError
      elif model == "add-1":
        if n==2:
          prob = mle_add_one (uni_count, bi_count, vocabs, ctx, ngram)
        elif n==3:
          prob = mle_add_one (bi_count, tri_count, vocabs, ctx, ngram)
        else:
          raise NotImplementedError
      elif model == "add-k":
        if n==2:
          prob = mle_add_k (uni_count, bi_count, vocabs, ctx, ngram)
        elif n==3:
          prob = mle_add_k (bi_count, tri_count, vocabs, ctx, ngram)
        else:
          raise NotImplementedError
      assert (not (prob == 0.0))
      f_model.write (ctx + ',' + w + ',' + str(math.log (prob)) + ',' + str(prob) + ',' + str(bi_count.get (ctx,0)) + ',' + str(tri_count.get (ngram, 0)))
      f_model.write ('\n')
      f_pmat.write (str(prob) + ',')
      f_cmat.write (str(tri_count.get (ngram, 0)) + ',')
    f_pmat.write ('\n')
    f_cmat.write ('\n')

  f_model.close()
  f_pmat.close()
  f_cmat.close()


def main ():
  parser = argparse.ArgumentParser ()
  parser.add_argument ('--ngram', type=int)
  parser.add_argument ('--input', type=str)
  parser.add_argument ('--output', type=str, default='model.txt')
  parser.add_argument ('--model', type=str, default='interpolation')
  parser.add_argument ('--maxtime', type=int, default=9999999999999) # max disproven time
  parser.add_argument ('--kind', type=str, default='all')

  args = parser.parse_args ()

  lines = get_lines (args.input)
  lines = list (filter (lambda line: float(line['disproven_time']) <= args.maxtime, lines))
  lines = list (filter (lambda line: args.kind == line['kind'], lines)) if args.kind != 'all' else lines

  raws = list (map (lambda line: line['raw'], lines))
  type_count = mk_type_count_map (raws) # [(typ1,32), (typ2,13), ...]
  type_rank = list(map (lambda x: x[0], type_count))
  print ('=== Appeared Types ===')
  pprint (type_count)
  print ('')
  sentences = preprocess (args.ngram, type_rank, raws) # Psudo words such as <S> and <E> are appended.
  # for i in range (5):
  #  print (raws[i] + ',' + sentences[i])
  word_count = mk_word_count (sentences)
  word_rank = sorted (word_count.items(), key=(lambda x:x[1]), reverse=True)

  unigrams = mk_ngram_sentences (1, sentences)
  bigrams = mk_ngram_sentences (2, sentences)
  trigrams = mk_ngram_sentences (3, sentences)
  quadgrams = mk_ngram_sentences (4, sentences)

  uni_count = mk_ngram_count (unigrams)
  bi_count = mk_ngram_count (bigrams)
  tri_count = mk_ngram_count (trigrams)
  quad_count = mk_ngram_count (quadgrams)

  uni_count = dict(sorted (uni_count.items(), key=(lambda x:x[1]), reverse=True))
  bi_count = dict(sorted (bi_count.items(), key=(lambda x:x[1]), reverse=True))
  tri_count = dict(sorted (tri_count.items(), key=(lambda x:x[1]), reverse=True))
  quad_count = dict(sorted (quad_count.items(), key=(lambda x:x[1]), reverse=True))

  f_uni = open ('./src/exploit/train/uni_count.txt', 'w')
  for (k,v) in uni_count.items():
    f_uni.write (k + ',' + str (v))
    f_uni.write ('\n')
  f_uni.close ()

  f_bi = open ('./src/exploit/train/bi_count.txt', 'w')
  for (k,v) in bi_count.items():
    f_bi.write (k + ',' + str (v))
    f_bi.write ('\n')
  f_bi.close ()

  f_tri = open ('./src/exploit/train/tri_count.txt', 'w')
  for (k,v) in tri_count.items():
    f_tri.write (k + ',' + str (v))
    f_tri.write ('\n')
  f_tri.close ()

  f_quad = open ('./src/exploit/train/quad_count.txt', 'w')
  for (k,v) in quad_count.items():
    f_quad.write (k + ',' + str (v))
    f_quad.write ('\n')
  f_quad.close ()

  vocabs = []
  for ngram in uni_count.keys():
    for w in ngram.split('-'):
      if w not in vocabs:
        vocabs.append (w)
  print ('=== Vocabs  ===')
  print (vocabs)
  print ('# Vocabs: ' + str (len(vocabs)))
  print ('')

  # learn_model (args.ngram, args.output, args.model, uni_count, bi_count, tri_count, quad_count, vocabs)

if __name__ == "__main__":
  main()
